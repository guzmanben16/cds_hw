{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Setting up Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For a neural net, we will use sigmoid as our activation function. The dataset is made of arrays of 1's and 0's and the expected output/ground truths are 1, 1, and 0. We will our neural net to have 4 input layers, 3 hidden layers, and 1 output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Input array\n",
    "X=np.array([[1,0,1,0],[1,0,1,1],[0,1,0,1]])\n",
    "\n",
    "#Output\n",
    "y=np.array([[1],[1],[0]])\n",
    "\n",
    "#sigmoid as activation function\n",
    "def sigmoid (x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "#derivative of sigmoid function\n",
    "def derivatives_sigmoid(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "input_layer = X.shape[1]\n",
    "hidden_layer = 3 \n",
    "output_layer = 1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Setting up weights and bias  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=5000 #iterate 5000 times\n",
    "lr=0.1 #learning rate is 0.01\n",
    "\n",
    "w=np.random.uniform(size=(input_layer,hidden_layer))\n",
    "b=np.random.uniform(size=(1,hidden_layer))\n",
    "w_out=np.random.uniform(size=(hidden_layer,output_layer))\n",
    "b_out=np.random.uniform(size=(1,output_layer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Training the Neural Net via Forward Propagation and Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in range(epoch):\n",
    "\n",
    "#Forward Propogation\n",
    "    hidden_layer_1=np.dot(X,w)\n",
    "    hidden_layer_input=hidden_layer_1 + b\n",
    "    hidden_layer_activation_fxn = sigmoid(hidden_layer_input)\n",
    "    output_layer_1=np.dot(hidden_layer_activation_fxn,w_out)\n",
    "    output_layer_input= output_layer_1+ b_out\n",
    "    output_pre = sigmoid(output_layer_input)\n",
    "    print(\"Values before backward propagation:\") #print initial outputs of neural net without backpropagation\n",
    "    print(output_pre)\n",
    "\n",
    "#Backpropagation\n",
    "    error = y-output\n",
    "    slope_output_layer = derivatives_sigmoid(output)\n",
    "    slope_hidden_layer = derivatives_sigmoid(hidden_layer_activation_fxn)\n",
    "    d_output = error * slope_output_layer\n",
    "    error_hidden_layer = d_output.dot(w_out.T)\n",
    "    d_hiddenlayer = error_hidden_layer * slope_hidden_layer\n",
    "    w_out += hidden_layer_activation_fxn.T.dot(d_output) *lr\n",
    "    b_out += np.sum(d_output, axis=0,keepdims=True) *lr\n",
    "    w += X.T.dot(d_hiddenlayer) *lr\n",
    "    b += np.sum(d_hiddenlayer, axis=0,keepdims=True) *lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values after backward propagation:\n",
      "[[0.99242405]\n",
      " [0.98810791]\n",
      " [0.01726999]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Values after backward propagation:\")\n",
    "print(output_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our neural network predicted the ground truth values of 1,1,0 as 0.99, 0.98, and 0.01. These values are very close to the ground truth and this show that our neural network is accurate. To confirm, since the outputs are continous variables, we will calculate mean squared error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00016568979252356682"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "predicted = [0.99242405, 0.98810791, 0.01726999]\n",
    "ground_truth = [1,1,0]\n",
    "\n",
    "eval_metric = mean_squared_error(ground_truth, predicted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Theoretically, a MSE closer to 0, the better it is. In this case, our overall MSE is 0.00016, which proves that our simple neural net is a good one. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
